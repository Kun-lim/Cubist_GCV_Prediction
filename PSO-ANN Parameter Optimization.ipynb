{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ba663f-ac44-486c-82c3-209220225ee2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This code realizes a process of tuning the parameters of a Multi-Layer Perceptron (MLP) neural network using Particle Swarm Optimization (PSO), and evaluates its performance on the task of predicting coal's Gross Calorific Value (GCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e986ad-6873-4cfa-b8d4-3d7e073a1c2a",
   "metadata": {},
   "source": [
    "## Steps：\n",
    "1.Import necessary libraries: pandas, sklearn, numpy, and pyswarm.\n",
    "\n",
    "2.Load the original coal data from a CSV file.\n",
    "\n",
    "3.Randomly sample 40% of the data.\n",
    "\n",
    "4.Select features (‘Moisture’, ‘Volatile_matter’, ‘Std.Ash’) and the target variable (‘GCV’) from the sampled data.\n",
    "\n",
    "5.Split the sampled data into training and testing sets, with a 20% testing size.\n",
    "\n",
    "6.Standardize the sampled training data using StandardScaler.\n",
    "\n",
    "7.Define an ANN model with a ReLU activation function and the Adam solver.\n",
    "\n",
    "8.Use Particle Swarm Optimization (PSO) to optimize the number of neurons in the hidden layers of the ANN.\n",
    "\n",
    "9.Train the ANN model with the best parameters found by PSO.\n",
    "\n",
    "10.Predict the GCV for the test set using the optimized ANN model.\n",
    "\n",
    "11.Calculate evaluation metrics (MAE, MSE, RMSE, R2, and MAPE) for the predictions on the sampled test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff937b-0bee-4905-95e4-d39ad03f0b7e",
   "metadata": {},
   "source": [
    "## Data Description\n",
    " The data being read from <coal_all.csv> contains 10 variables：\n",
    " \n",
    "(Moisture, Volatile_matter, Fixed_Carbon, Ash, Hydrogen, Carbon, Nitrogen, Oxygen, Sulfur, GCV)\n",
    "\n",
    " Three sets of data (SET1, SET2, SET3) have been constructed based on different input variables.\n",
    "\n",
    " SET1 (Moisture, Volatile_matter, Ash, GCV)\n",
    " \n",
    " SET2 (Carbon, Oxygen, Sulfur, GCV)\n",
    " \n",
    " SET3 (Moisture, Volatile_matter, Ash, Hydrogen, Carbon, Oxygen, Sulfur, GCV)\n",
    "\n",
    "In the following code, the SET1 sub-dataset is used. To utilize the other two datasets, simply specify the corresponding variable names in the statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaeec10-d3b9-4f27-8a12-2f66193a5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1773b90-346e-4269-89e9-cfaf20615526",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "Due to the low performance of my computer, if all the original data are added to the optimization algorithm, it will take several days to complete. To improve the running efficiency, a scheme was adopted to randomly select a portion of data from the original data to optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "359c4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the original data\n",
    "data_original = pd.read_csv(\"F:/煤质论文2/coal_all.csv\")\n",
    "\n",
    "# Sample 40% of the data randomly\n",
    "sampled_data_original = data_original.sample(frac=0.4, random_state=42)\n",
    "\n",
    "# Select features and target from the sampled data\n",
    "X_sampled = sampled_data_original[['Moisture', 'Volatile_matter', 'Std.Ash']]\n",
    "y_sampled = sampled_data_original['GCV']\n",
    "\n",
    "# Split the sampled data into training and testing sets\n",
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the sampled training data\n",
    "scaler = StandardScaler()\n",
    "X_train_sampled_scaled = scaler.fit_transform(X_train_sampled)\n",
    "X_test_sampled_scaled = scaler.transform(X_test_sampled)\n",
    "print(X_train_sampled_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d1238-a728-48f4-8aba-3f0bb694d42b",
   "metadata": {},
   "source": [
    "## Use PSO for parameter optimization\n",
    "mainly to optimize the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba557e0-d809-4969-9fae-9f348855db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO-ANN\n",
    "from pyswarm import pso\n",
    "def ann_obj_function(params):\n",
    "   # Create an ANN model\n",
    "    ann = MLPRegressor(hidden_layer_sizes=(int(params[0]), int(params[1])), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "    ann.fit(X_train_sampled_scaled, y_train_sampled)\n",
    "    y_pred = ann.predict(X_test_sampled)\n",
    "    mse = mean_squared_error(y_test_sampled, y_pred)\n",
    "    return mse\n",
    "# Perform parameter optimization using Particle Swarm Optimization (PSO)\n",
    "lb = [3, 3]  # Lower bound for the number of neurons in each hidden layer\n",
    "ub = [24, 24]  # Upper bound for the number of neurons in each hidden layer\n",
    "# Define the dimensions of the PSO as 2, because we have two parameters to optimize\n",
    "xopt, fopt = pso(ann_obj_function, lb, ub)\n",
    "\n",
    "# Create a model using the optimized parameters\n",
    "optimized_neurons = (int(xopt[0]), int(xopt[1]))\n",
    "ann_best = MLPRegressor(hidden_layer_sizes=optimized_neurons, \n",
    "                       activation='relu', solver='adam', max_iter=200, random_state=42)\n",
    "ann_best.fit(X_train_sampled_scaled, y_train_sampled)\n",
    "\n",
    "# Print the optimized parameters and results\n",
    "optimized_params = \", \".join(f\"{param}\" for param in xopt)\n",
    "print(f\"Optimized parameters: {optimized_params}\")\n",
    "print(f\"Best MSE: {fopt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906343d6-1377-48a3-9b94-4860349f11c2",
   "metadata": {},
   "source": [
    "## GCV prediction is made by using the determined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_best.fit(X_train_sampled_scaled,  y_train_sampled)\n",
    "pso_ann_pred = ann_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b28ee-3bff-4417-bfdc-ed8cb8636ba4",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412491f-2c06-4d83-9c99-5003b6ca1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "def calculate_mape(actual, predicted):\n",
    "    return 100 * abs((actual - predicted).sum() / actual.sum())\n",
    "# Calculate evaluation metrics using the sampled test data\n",
    "pso_ann_mae_sampled = mean_absolute_error(y_test_sampled, pso_ann_pred_sampled)\n",
    "pso_ann_mse_sampled = mean_squared_error(y_test_sampled, pso_ann_pred_sampled)\n",
    "pso_ann_r2_sampled = r2_score(y_test_sampled, pso_ann_pred_sampled)\n",
    "pso_ann_rmse_sampled = math.sqrt(pso_ann_mse_sampled)\n",
    "pso_ann_mape_sampled = calculate_mape(y_test_sampled, pso_ann_pred_sampled)\n",
    "\n",
    "pso_ann_mae_sampled, pso_ann_mse_sampled, pso_ann_rmse_sampled, pso_ann_r2_sampled, pso_ann_mape_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8ba04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
